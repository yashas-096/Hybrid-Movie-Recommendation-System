# -*- coding: utf-8 -*-
"""interface_hybrid_recommendation_system_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13I2FGEXZEFWG79KfUnYWHgJ4U-ef5QLW 
"""

# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import linear_kernel

# Commented out IPython magic to ensure Python compatibility.
import os
import time

# data science imports
import math
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors

# utils import
from fuzzywuzzy import fuzz

# visualization imports
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use('ggplot')

# %matplotlib inline

df = pd.read_csv("movies-preprocessed-final.csv")
df['index'] = df.index
df_movies = pd.read_csv(
    'movies-preprocessed-final.csv',
    usecols=['IMDb-MovieId', 'Movie-Title'],
    dtype={'IMDb-MovieId': 'str', 'Movie-Title': 'str'})
df_ratings = pd.read_csv(
    'ratings_preprocessed3.csv',
    usecols=['UserID', 'MovieID', 'Rating'],
    dtype={'UserID': 'str', 'MovieID': 'str', 'Rating': 'float32'})
dataset = pd.read_csv("tweets-final-agg-rating.csv")

"""# Content Based Recommendation"""

# Choosing our feature set for content based movie recommendation system
features = ['Movie-Title', 'Genre', 'Writer', 'Director', 'Actor']
# Checking if any NaN values exist in the selected dataframe
df[features].isnull().values.any()
# we need to clean and preprocess the data for use. So, we will fill all the NaN values with blank string in the dataframe
for feature in features:
    df[feature] = df[feature].fillna('')

#Function to combine the features, Call this function over each row of the dataframe.
def combine_features(row):
    return row['Movie-Title']+' '+row['Genre']+' '+row['Writer']+' '+row['Director']+' '+row['Actor']

# Apply combine_feature method over each row of Dataframe to store the combined string in "combined_features" column
df['combined_features'] = df.apply(combine_features, axis = 1)
df['combined_features'] = df['combined_features'].str.replace(',',' ')
df.head()

# print(df.loc[0, 'combined_features'])

# Define two helper functions to get movie title from movie index and vice-versa
def get_title_from_index(index):
    return df[df.index == index]["Movie-Title"].values[0]
def get_index_from_title(title):
    return df[df["Movie-Title"] == title]["index"].values[0]
def get_Tmdb_id(title):
    return df[df["Movie-Title"] == title]["TMDb-MovieId"].values[0]
def get_Imdb_id(title):
    return df[df["Movie-Title"] == title]["IMDb-MovieId"].values[0]

# Content-based filtering model
def get_content_based_recommendations(query_title,n):
    # compute metadata similarity scores here using content-based techniques
    # Using tfidf Vectoriser
    tfidf_vector = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf_vector.fit_transform(df['combined_features'])
    sim_matrix = linear_kernel(tfidf_matrix,tfidf_matrix)
    movie_index = get_index_from_title(query_title)
    similar_movies = list(enumerate(sim_matrix[movie_index]))
    """
    We will sort the list similar_movies according to similarity scores in descending order.
    Since the most similar movie to a given movie will be itself, 
    we will discard the first element after sorting the movies.
    """
    sorted_similar_movies = sorted(similar_movies,key=lambda x:x[1],reverse=True)[1:]
    result = []
    i=0
    # print("\n\n\nContent-based Recommendation: \n\nTop "+ str(n) +" similar movies to "+ query_title +" are:\n")
    for element in sorted_similar_movies:
        movie = get_title_from_index(element[0])
        # print(movie)
        scoreSet = (movie,element[1])
        result.append(scoreSet)
        i=i+1
        if i>n:
            break
    # print("\n")
    return result

    # return [recommended_movie_1, recommended_movie_2, ...]
query_title = "Wonder Woman"
result1 = get_content_based_recommendations(query_title,5)
# print(result1)
result2 = get_content_based_recommendations(query_title,10)
# print(result2)

"""# Collaborative filtering Recommendation"""

df_movies.rename(columns = {'Movie-Title':'title'}, inplace = True)
df_movies.rename(columns = {'IMDb-MovieId':'MovieID'}, inplace = True)

# pivot and create movie-user matrix
movie_user_mat = df_ratings.pivot(index='MovieID', columns='UserID', values='Rating').fillna(0)
# create mapper from movie title to index
movie_to_idx = {
    movie: i for i, movie in 
    enumerate(list(df_movies.set_index('MovieID').loc[movie_user_mat.index].title))
}
# transform matrix to scipy sparse matrix
movie_user_mat_sparse = csr_matrix(movie_user_mat.values)

# Commented out IPython magic to ensure Python compatibility.
# %env JOBLIB_TEMP_FOLDER=/tmp
# define model
model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)
# fit
model_knn.fit(movie_user_mat_sparse)

def fuzzy_matching(mapper, fav_movie, verbose=True):
    """
    return the closest match via fuzzy ratio. If no match found, return None
    
    Parameters
    ----------    
    mapper: dict, map movie title name to index of the movie in data

    fav_movie: str, name of user input movie
    
    verbose: bool, print log if True

    Return
    ------
    index of the closest match
    """
    match_tuple = []
    # get match
    for title, idx in mapper.items():
        ratio = fuzz.ratio(title.lower(), fav_movie.lower())
        if ratio >= 60:
            match_tuple.append((title, idx, ratio))
    # sort
    match_tuple = sorted(match_tuple, key=lambda x: x[2])[::-1]
    if not match_tuple:
        print('Oops! No match is found')
        return
    if verbose:
        print('Found possible matches in our database: {0}\n'.format([x[0] for x in match_tuple]))
    return match_tuple[0][1]

def make_recommendation(model_knn, data, mapper, fav_movie, n_recommendations):
    """
    return top n similar movie recommendations based on user's input movie


    Parameters
    ----------
    model_knn: sklearn model, knn model

    data: movie-user matrix

    mapper: dict, map movie title name to index of the movie in data

    fav_movie: str, name of user input movie

    n_recommendations: int, top n recommendations

    Return
    ------
    list of top n similar movie recommendations
    """

    # get input movie index
    #print('You have input movie:', fav_movie)
    idx = fuzzy_matching(mapper, fav_movie, verbose=True)
    # inference
    # print('\n\nCollaborative Recommendation system starting to make inference')
    # print('......\n')
    # print('\nTop', n_recommendations, 'similar movies to ', fav_movie,' are:\n\n')
    distances, indices = model_knn.kneighbors(data[idx], n_neighbors=n_recommendations+1)
    # get list of raw idx of recommendations
    raw_recommends = \
        sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())), key=lambda x: x[1])[1:]
    # get reverse mapper
    reverse_mapper = {v: k for k, v in mapper.items()}
    # print recommendations
    result = []
    # print('Recommendations for {}:'.format(fav_movie))
    for i, (idx, dist) in enumerate(raw_recommends):
        movie = reverse_mapper[idx]
        scoreSet = (movie,1-dist)
        # print(movie)
        # print('{0}: {1}, with distance of {2}'.format(i+1, movie, dist))
        result.append(scoreSet)
    return result

# Collaborative filtering model (using the code you provided)
def get_collaborative_recommendations(query_title,n):
    recommendations = []
    recommendations = make_recommendation(model_knn=model_knn,data=movie_user_mat_sparse,fav_movie=query_title,mapper=movie_to_idx,n_recommendations=n)
    return recommendations
query_title = "Wonder Woman"
result1 = get_collaborative_recommendations(query_title,5)
#print(result1)
result2 = get_collaborative_recommendations(query_title,10)
#print(result2)

"""# Hybrid Recommendation System"""

# Hybrid model
# def get_hybrid_recommendations(query_title,n):
#     collaborative_recommendations = get_collaborative_recommendations(query_title,n)
#     content_based_recommendations = get_content_based_recommendations(query_title,n)

#     hybrid_scores = {}
#     for movie in set(collaborative_recommendations + content_based_recommendations):
#         collaborative_score = movie[1]
#         content_based_score = movie[1]

#         hybrid_score = collaborative_score + content_based_score
#         hybrid_scores[movie[0]] = hybrid_score
 
#     sorted_hybrid= [(movie_title,score) for movie_title, score in sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)]
#     movie = [title for title, score in sorted_hybrid]
#     return movie


def get_hybrid_recommendation(query_title, alpha, n):
    collaborative_recommendations = get_collaborative_recommendations(query_title, n)
    content_based_recommendations = get_content_based_recommendations(query_title, n)

    hybrid_scores = {}
    for movie in set(collaborative_recommendations + content_based_recommendations):
        collaborative_score = movie[1]
        content_based_score = movie[1]

        hybrid_score = alpha * collaborative_score + (1 - alpha) * content_based_score
        hybrid_scores[movie[0]] = hybrid_score

    sorted_hybrid = [(movie_title, score) for movie_title, score in sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)][:n]

    return sorted_hybrid

# Example usage
# query_title = "Wonder Woman"
# hybrid_recommendations = get_hybrid_recommendations(query_title,10)
# #print(hybrid_recommendations)
# print('Hybrid recommendations for ',query_title,':\n\n')
# for i in hybrid_recommendations:
#     print(i)



def compute_sentiment_similarity(movie_title, D, n):
    # Get the sentiment rating for the given movie
    query_rating = dataset.loc[dataset['Movie'] == movie_title]['Aggregate-Rating'].iloc[0]
    similarity_scores = {}
    # Compute the sentiment similarity between the given movie and all other movies in the dataset
    for _, row in dataset.iterrows():
        title = row['Movie']
        if title == movie_title:
            continue
        other_rating = row['Aggregate-Rating']
        similarity_score = D - abs(query_rating - other_rating)
        similarity_scores[title] = similarity_score

    similarity_scores = [(movie_title, rating) for movie_title, rating in sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)]

    return similarity_scores


# Get Weighted Score Fusion Recommendations
def get_weighted_scores(query_title, alpha, n):
    sentiment_score = compute_sentiment_similarity(query_title, 10, 10)
    sentiment_result = sentiment_score
    sorted_hybrid = get_hybrid_recommendation(query_title, alpha, n)
    final_score = []

    for tuple1 in sorted_hybrid:
      for tuple2 in sentiment_result:
        if tuple1[0] == tuple2[0]:
          final_score.append((tuple1[0], 0.5*tuple1[1]+0.5*tuple2[1]))

    final_score.sort(key=lambda x: x[1], reverse=True)
    recommendation = [title for title, score in final_score]
    return recommendation


# query_title = "The Maze Runner"
# alpha = 0.5
# recommendations = get_weighted_scores(query_title, alpha, 10)
# print(recommendations)
# print('Weighted recommendations for ', query_title, ':\n\n')
# for i in recommendations:
#     print(i)


"""# Getting Built In Recommendations from Tmdb website"""

import requests,json,csv,os

#built in recommendations from tmdb
def get_tmdb_recommendations(query_title):
   movieId = get_Tmdb_id(query_title)
   query = 'https://api.themoviedb.org/3/movie/' + str(movieId) + '/recommendations?api_key=0270df115f1eb09b6c87a6a3a1230316&language=en-US&page=1'
   response =  requests.get(query)
   recommendations = []
   if response.status_code==200: 
      #status code ==200 indicates the API query was successful
      array = response.json()
      text = json.dumps(array)
      dataset = json.loads(text)
      # print(dataset)
      try:
          for m in dataset['results']:
              recommendations.append(str(m['title']))
      except:
          print("error  in results")
   else:
      print("error in getting response")

   recommendations=recommendations[:-1]
   return recommendations

# query_title = "Wonder Woman"
# tmdbrec = get_tmdb_recommendations(query_title)
# print('TMDb recommendations for ',query_title,':\n\n')
# for i in tmdbrec:
#     print(i)
#print(result)

"""# Evaluating the Hybrid model using precision after spliting the dataset """

# common = set(recommendations).intersection(tmdbrec)
# n=len(common)
# prec=n/10
# print(prec)

import warnings
warnings.filterwarnings('ignore')

